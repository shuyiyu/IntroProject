{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initial setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_full = pd.read_json('yelp_academic_dataset_business.json', lines = True)\n",
    "tip = pd.read_json('yelp_academic_dataset_tip.json', lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "In this section, we will transform the Yelp dataset and produce the format required for data mining and modeling through the following steps:\n",
    "\n",
    "- Filter to U.S. restaurants that were opened in 2016\n",
    "- Flatten attributes and categories\n",
    "- Transform categorical variables into dummy variables\n",
    "- Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter to U.S. restaurants that were opened in 2016\n",
    "# 1) Filter to U.S. businesses\n",
    "zip_crosswalk = pd.read_csv('zip_crosswalk.csv')\n",
    "zip_crosswalk = zip_crosswalk.drop_duplicates(subset = ['ZIP', 'STATE'], keep = False)[['ZIP', 'STATE']]\n",
    "# Transform digit-only postal code to int\n",
    "digit_only = df_full['postal_code'].map(lambda x: True if all(i.isdigit() for i in x) else False)\n",
    "df_full = df_full[(digit_only) & (df_full['postal_code'] != '')]\n",
    "df_full['postal_code_int'] = [int(x) for x in df_full['postal_code']]\n",
    "# Join crosswalk by zip and state\n",
    "df_us = pd.merge(df_full, zip_crosswalk[['ZIP', 'STATE']], \n",
    "              left_on = ['postal_code_int', 'state'],\n",
    "              right_on = ['ZIP', 'STATE'])\n",
    "\n",
    "# 2) Filter to restaurants\n",
    "df_us = df_us[df_us['categories'].str.contains('Restaurants|Food', na = False)]\n",
    "df_us.set_index('business_id')\n",
    "\n",
    "# 3) Filter to restaurants that were opened in 2016\n",
    "start = '2016-01-01'\n",
    "end = '2016-12-31'\n",
    "tip = tip.groupby('business_id')['date'].min().reset_index()\n",
    "tip = tip[(tip['date'] >= start) & (tip['date'] <= end)]\n",
    "df = pd.merge(tip, df_us, on = 'business_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check geological distribution\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "df.groupby('state')['business_id'].count().plot(kind = 'bar', color = '#0066cc', width = 0.8, linewidth = 1, edgecolor = 'black')\n",
    "plt.xlabel('U.S. State', fontsize = 15)\n",
    "plt.ylabel('Restaurant Count', fontsize = 15)\n",
    "plt.xticks(fontsize = 15, rotation = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop states with few data points\n",
    "state_list = ['AZ', 'NC', 'NV', 'OH', 'PA', 'WI']\n",
    "print('Total data points in df:', len(df))\n",
    "df = df[np.in1d(df['state'], state_list)].reset_index()\n",
    "print('Total data points in df after removing a few states:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check restaurant closure\n",
    "df.groupby('is_open')['business_id'].count().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten attributes and categories\n",
    "# 1) Categories\n",
    "from collections import Counter\n",
    "# Extract categories from list of lists\n",
    "cat = ', '.join(df['categories'])\n",
    "# Count frequency\n",
    "counter = Counter(cat.split(', '))\n",
    "# Append categories as dummies\n",
    "for cat in list(counter.keys()):\n",
    "    df[cat] = np.where(df['categories'].str.contains(cat), 1, 0)\n",
    "df.drop('categories', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2) Attributes\n",
    "# Unuseful attributes to be dropped\n",
    "drop_list = ['AcceptsInsurance', 'AgesAllowed', 'Ambience', 'DietaryRestrictions',\n",
    "             'BestNights', 'BikeParking', 'BusinessAcceptsBitcoin', 'Music', 'ByAppointmentOnly',\n",
    "             'CoatCheck', 'DogsAllowed' , 'DriveThru', 'GoodForDancing', 'Smoking', 'BusinessParking', 'GoodForMeal']\n",
    "# Flattening\n",
    "att = df['attributes'].apply(pd.Series)\n",
    "meal = att['GoodForMeal'].fillna(\"{'dessert': False, 'latenight': False, 'lunch': False, 'dinner': False, 'breakfast': False, 'brunch': False}\")\n",
    "parking = att['BusinessParking'].fillna(\"{'garage': False, 'street': True, 'validated': False, 'lot': False, 'valet': False}\")\n",
    "parking_list = ['garage', 'street', 'validated', 'lot', 'valet']\n",
    "meal_list = ['dessert', 'latenight', 'lunch', 'dinner', 'breakfast', 'brunch']\n",
    "\n",
    "# Define a function to flatten json lists\n",
    "import ast\n",
    "def flatten_json(df, col):\n",
    "    for i in range(len(col)):\n",
    "        json_dict = ast.literal_eval(col[i])\n",
    "        for j in json_dict.keys():\n",
    "            df[j] = np.where(json_dict[j], 1, 0) \n",
    "            \n",
    "flatten_json(df = att, col = parking)\n",
    "flatten_json(df = att, col = meal)\n",
    "\n",
    "# Handle missing value in attributes\n",
    "att['Alcohol'].fillna('none', inplace = True)\n",
    "att['NoiseLevel'].fillna('average', inplace = True)\n",
    "att['RestaurantsAttire'].fillna('casual', inplace = True)\n",
    "att['WiFi'].fillna('no', inplace = True)\n",
    "att['RestaurantsPriceRange2'].fillna(0, inplace = True)\n",
    "\n",
    "# Transform attributes into dummy variables\n",
    "att = att.drop(drop_list, axis = 1).fillna(value = False)\n",
    "att = pd.get_dummies(att, drop_first = True)\n",
    "\n",
    "# Append attributes to main dataframe\n",
    "df = pd.merge(df, att, left_index = True, right_index = True)\n",
    "df.drop('attributes', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. External Data\n",
    "In this section, we will integrate the following external data sources to the Yelp dataset:\n",
    "- Zillow property price data\n",
    "- Demographic data by zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zillow\n",
    "zillow = pd.read_csv('zillow_median_price.csv', encoding='cp1252')\n",
    "# Extract zipcodes\n",
    "zipcodes = df['postal_code'].unique()\n",
    "df['postal_code_int'] = [int(i) for i in df['postal_code']]\n",
    "house_price = zillow[np.in1d(zillow['RegionName'], \n",
    "                             df['postal_code_int'])][['RegionName', '2016-01']]\n",
    "house_price.columns = ['postal_code_int', 'median_sqft_price']\n",
    "# Append median house price to main df\n",
    "df = pd.merge(df, house_price, on = 'postal_code_int', how = 'left')\n",
    "df['median_sqft_price'] = df['median_sqft_price'].fillna(df['median_sqft_price'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add zip-level demographics\n",
    "from uszipcode import SearchEngine\n",
    "keys = ['zipcode', 'housing_units', 'land_area_in_sqmi', 'median_home_value', 'median_household_income', \n",
    "        'occupied_housing_units', 'population', 'population_density', 'annual_individual_earnings', \n",
    "        'educational_attainment_for_population_25_and_over', 'employment_status', 'families_vs_singles', \n",
    "        'households_with_kids', 'housing_occupancy', 'means_of_transportation_to_work_for_workers_16_and_over', \n",
    "        'population_by_age', 'population_by_gender', 'population_by_race', 'travel_time_to_work_in_minutes']\n",
    "keep_col = ['housing_units', 'land_area_in_sqmi', 'median_home_value', 'median_household_income', \n",
    "            'occupied_housing_units', 'population', 'population_density', 'postal_code']\n",
    "lst = []\n",
    "for zipcode in zipcodes:\n",
    "    search = SearchEngine(simple_zipcode=False) \n",
    "    item = search.by_zipcode(zipcode)\n",
    "    newDict  = item.to_dict()\n",
    "    lst.append([newDict.get(key) for key in keys])\n",
    "keys[0] = 'postal_code'\n",
    "df_demograph = pd.DataFrame(lst, columns= keys)[keep_col]\n",
    "df = pd.merge(df, df_demograph, on = 'postal_code', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Feature Engineering\n",
    "In this section, we will perform feature engineering to extract the following attributes:\n",
    "- Restaurant density and competition level in the neighborhood\n",
    "- Whether a given restaurant belong to a local/national chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Restaurant density\n",
    "import geopy.distance\n",
    "business_id = []\n",
    "density = []\n",
    "avg_stars = []\n",
    "avg_review_count = []\n",
    "std_stars = []\n",
    "std_review_count = []\n",
    "\n",
    "# Create a random sample from all U.S. restaurants (n = 10000) to calculate restaurant density\n",
    "df_us = df_us[np.in1d(df_us['state'], state_list)]\n",
    "df_us = df_us.dropna(subset = ['latitude', 'longitude'], axis = 0).reset_index()\n",
    "sample_us = df_us.drop_duplicates(subset=['latitude', 'longitude', 'stars', 'review_count']).sample(n = 10000).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    print('Looping over', i, '...')\n",
    "    coord1 = [df.at[i, 'latitude'], df.at[i, 'longitude']]\n",
    "    count = 1\n",
    "    stars = 0\n",
    "    review_count = 0\n",
    "    for j in range(len(sample_us)):\n",
    "        coord2 = [sample_us.at[j, 'latitude'], sample_us.at[j, 'longitude']]\n",
    "        try:\n",
    "            distance = geopy.distance.vincenty(coord1, coord2).miles\n",
    "            if distance < 1:\n",
    "                count = count + 1\n",
    "                stars = stars + sample_us.at[j, 'stars']\n",
    "                review_count = review_count + sample_us.at[j, 'review_count']\n",
    "        except:\n",
    "            print('Error!')\n",
    "    business_id.append(df.at[i, 'business_id'])\n",
    "    density.append(count)\n",
    "    avg_stars.append(stars/count)\n",
    "    avg_review_count.append(review_count/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biz_density = pd.DataFrame({'business_id': business_id,\n",
    "                            'density' : density,\n",
    "                            'avg_stars' : avg_stars,\n",
    "                            'avg_review_count' : avg_review_count})\n",
    "df = pd.merge(df, biz_density, on = 'business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify whether a given restaurant is a local or national chain\n",
    "local_chain = df.groupby(['state', 'name'])['business_id'].count().reset_index()\n",
    "local_chain.columns = ['state', 'name', 'count']\n",
    "local_chain['is_local_chain'] = np.where(local_chain['count'] > 1, 1, 0)\n",
    "\n",
    "national_chain = local_chain.groupby('name')['state'].count().reset_index()\n",
    "national_chain.columns = ['name', 'count']\n",
    "national_chain['is_national_chain'] = np.where(national_chain['count'] > 2, 1, 0)\n",
    "\n",
    "df = pd.merge(df, local_chain[['name', 'state', 'is_local_chain']], on = ['state','name'], how = 'left')\n",
    "df = pd.merge(df, national_chain[['name', 'is_national_chain']], on = ['name'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "# pd.set_option(\"display.max_columns\",500)\n",
    "# Drop non-features\n",
    "drop_col = ['index', 'business_id', 'date', 'address', 'hours', \n",
    "            'latitude','longitude','name','review_count','stars',\n",
    "            'neighborhood', 'postal_code', 'postal_code_int', 'ZIP', 'STATE']\n",
    "# Drop zipcide with missing value\n",
    "df_clean = df[(df['postal_code'] != '89158') & (df['postal_code'] != '85378')].drop(drop_col, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert categorical variables to dummies\n",
    "dummy_col = ['city', 'state']\n",
    "df_clean = pd.get_dummies(data = df_clean, columns = dummy_col).dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_open\n",
       "0     485\n",
       "1    2324\n",
       "Name: is_open, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resampling\n",
    "df_clean.groupby('is_open')['is_open'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "X = df_clean.drop('is_open', axis = 1)\n",
    "y = np.where(df_clean['is_open'] == 1, 0, 1)\n",
    "\n",
    "# Scale Transform X\n",
    "cols = X.columns\n",
    "trans = RobustScaler().fit(X)\n",
    "X = pd.DataFrame(trans.transform(X))\n",
    "X.columns = cols\n",
    "\n",
    "# Perform stratified 4-fold\n",
    "k = 4\n",
    "skf = StratifiedKFold(n_splits = k, shuffle = True, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Logistic regression (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Baseline: logitstic regression with regularization + grid search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics\n",
    "\n",
    "auc = []\n",
    "pr_auc = []\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Upsample minority class\n",
    "    X_close = X_train.iloc[y_train == 1, :]\n",
    "    X_open = X_train.iloc[y_train == 0, :]\n",
    "    y_close = y_train[y_train == 1]\n",
    "    y_open = y_train[y_train == 0]\n",
    "    X_close, y_close = resample(X_close, y_close, n_samples = len(X_open), random_state = 123)\n",
    "    X_train = pd.concat([X_close, X_open])\n",
    "    y_train = np.concatenate((y_close, y_open))\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict_proba(X_test)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred[:, 1])\n",
    "    auc.append(metrics.auc(fpr, tpr))\n",
    "    pr_auc.append(metrics.average_precision_score(y_test, y_pred[:, 1]))\n",
    "\n",
    "print('The average PR-AUC is', np.mean(pr_auc))\n",
    "print('The standard deviation is', np.std(pr_auc))\n",
    "print('The average AUC is', np.mean(auc))\n",
    "print('The standard deviation is', np.std(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic regression with SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "auc = []\n",
    "pr_auc = []\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Upsample minority class in training data using SMOTE\n",
    "    X_train, y_train = SMOTE(random_state = 123).fit_resample(X_train, y_train)\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict_proba(X_test)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred[:, 1])\n",
    "    auc.append(metrics.auc(fpr, tpr))\n",
    "    pr_auc.append(metrics.average_precision_score(y_test, y_pred[:, 1]))\n",
    "\n",
    "print('The average PR-AUC is', np.mean(pr_auc))\n",
    "print('The standard deviation is', np.std(pr_auc))\n",
    "print('The average AUC is', np.mean(auc))\n",
    "print('The standard deviation is', np.std(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Logistic regression with regularization grid search\n",
    "auc = []\n",
    "pr_auc = []\n",
    "penalty = []\n",
    "reg = []\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Upsample minority class in training data\n",
    "    X_close = X_train.iloc[y_train == 1, :]\n",
    "    X_open = X_train.iloc[y_train == 0, :]\n",
    "    y_close = y_train[y_train == 1]\n",
    "    y_open = y_train[y_train == 0]\n",
    "    X_close, y_close = resample(X_close, y_close, n_samples = len(X_open))\n",
    "    X_train = pd.concat([X_close, X_open])\n",
    "    y_train = np.concatenate((y_close, y_open))\n",
    "    \n",
    "    for p in ['l1', 'l2']:\n",
    "        for c in [10**i for i in range(-3, 3)]:\n",
    "            lr = LogisticRegression(penalty = p, C = c)\n",
    "            lr.fit(X_train, y_train)\n",
    "            y_pred = lr.predict_proba(X_test)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred[:, 1])\n",
    "            auc.append(metrics.auc(fpr, tpr))\n",
    "            pr_auc.append(metrics.average_precision_score(y_test, y_pred[:, 1]))\n",
    "            penalty.append(p)\n",
    "            reg.append(c)\n",
    "\n",
    "auc_df_lr = pd.DataFrame({'penalty' : penalty,\n",
    "                       'C': reg,\n",
    "                       'auc' : auc})\n",
    "auc_df_lr.groupby(['penalty', 'C']).agg({'auc' : ['mean', 'std']}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Logistic regression with regularization grid search + SMOTE\n",
    "auc = []\n",
    "penalty = []\n",
    "reg = []\n",
    "pr_auc = []\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Upsample minority class in training data using SMOTE\n",
    "    X_train, y_train = SMOTE(random_state = 123).fit_resample(X_train, y_train)\n",
    "    \n",
    "    for p in ['l1', 'l2']:\n",
    "        for c in [10**i for i in range(-3, 3)]:\n",
    "            lr = LogisticRegression(penalty = p, C = c)\n",
    "            lr.fit(X_train, y_train)\n",
    "            y_pred = lr.predict_proba(X_test)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred[:, 1])\n",
    "            auc.append(metrics.auc(fpr, tpr))\n",
    "            pr_auc.append(metrics.average_precision_score(y_test, y_pred[:, 1]))\n",
    "            penalty.append(p)\n",
    "            reg.append(c)\n",
    "\n",
    "auc_df_lr = pd.DataFrame({'penalty' : penalty,\n",
    "                       'C': reg,\n",
    "                       'auc' : auc})\n",
    "auc_df_lr.groupby(['penalty', 'C']).agg({'auc' : ['mean', 'std']}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "auc = []\n",
    "n_tree = []\n",
    "max_features = []\n",
    "criterion = []\n",
    "max_depth = []\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Upsample minority class in training data\n",
    "    X_close = X_train.iloc[y_train == 1, :]\n",
    "    X_open = X_train.iloc[y_train == 0, :]\n",
    "    y_close = y_train[y_train == 1]\n",
    "    y_open = y_train[y_train == 0]\n",
    "    X_close, y_close = resample(X_close, y_close, n_samples = len(X_open), random_state = 123)\n",
    "    X_train = pd.concat([X_close, X_open])\n",
    "    y_train = np.concatenate((y_close, y_open))\n",
    "    \n",
    "    for n in [50, 100, 150, 200]:\n",
    "        for m in ['sqrt', 'log2']:\n",
    "            for c in ['gini', 'entropy']:\n",
    "                for d in [int(d) for d in np.linspace(10, 100, num = 10)]:\n",
    "                    rf = RandomForestClassifier(n_estimators = n, max_features = m, criterion = c, max_depth = d)\n",
    "                    rf.fit(X_train, y_train)\n",
    "                    y_pred = rf.predict_proba(X_test)\n",
    "                    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred[:, 1])\n",
    "                    auc.append(metrics.auc(fpr, tpr))\n",
    "                    n_tree.append(n)\n",
    "                    max_features.append(m)\n",
    "                    criterion.append(c)\n",
    "                    max_depth.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "auc_df_rf = pd.DataFrame({'n_tree' : n_tree,\n",
    "                       'max_features': max_features,\n",
    "                       'criterion' : criterion,\n",
    "                       'max_depth' : max_depth,\n",
    "                       'auc' : auc})\n",
    "auc_df_rf.groupby(['n_tree', 'max_features', 'criterion', 'max_depth']).agg({'auc' : ['mean', 'std']}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc = []\n",
    "n_tree = []\n",
    "max_features = []\n",
    "criterion = []\n",
    "max_depth = []\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Upsample minority class in training data using SMOTE\n",
    "    X_train, y_train = SMOTE(random_state = 123).fit_resample(X_train, y_train)\n",
    "    \n",
    "    for n in [50, 100, 150, 200]:\n",
    "        for m in ['sqrt', 'log2']:\n",
    "            for c in ['gini', 'entropy']:\n",
    "                for d in [int(d) for d in np.linspace(10, 100, num = 10)]:\n",
    "                    rf = RandomForestClassifier(n_estimators = n, max_features = m, criterion = c, max_depth = d)\n",
    "                    rf.fit(X_train, y_train)\n",
    "                    y_pred = rf.predict_proba(X_test)\n",
    "                    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred[:, 1])\n",
    "                    auc.append(metrics.auc(fpr, tpr))\n",
    "                    n_tree.append(n)\n",
    "                    max_features.append(m)\n",
    "                    criterion.append(c)\n",
    "                    max_depth.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc_df_rf = pd.DataFrame({'n_tree' : n_tree,\n",
    "                       'max_features': max_features,\n",
    "                       'criterion' : criterion,\n",
    "                       'max_depth' : max_depth,\n",
    "                       'auc' : auc})\n",
    "auc_df_rf.groupby(['n_tree', 'max_features', 'criterion', 'max_depth']).agg({'auc' : ['mean', 'std']}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics\n",
    "\n",
    "auc = []\n",
    "n_tree = []\n",
    "learning_rate = []\n",
    "max_depth = []\n",
    "gamma = []\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Upsample minority class in training data\n",
    "    X_close = X_train.iloc[y_train == 1, :]\n",
    "    X_open = X_train.iloc[y_train == 0, :]\n",
    "    y_close = y_train[y_train == 1]\n",
    "    y_open = y_train[y_train == 0]\n",
    "    X_close, y_close = resample(X_close, y_close, n_samples = len(X_open), random_state = 123)\n",
    "    X_train = pd.concat([X_close, X_open])\n",
    "    y_train = np.concatenate((y_close, y_open))\n",
    "    \n",
    "    for n in [50, 100, 150, 200]:\n",
    "        for i in [0.001, 0.01, 0.1, 0.2]:\n",
    "            xgb = XGBClassifier(n_estimators = n, learning_rate = i)\n",
    "            xgb.fit(X_train, y_train)\n",
    "            y_pred = xgb.predict_proba(X_test)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred[:, 1])\n",
    "            auc.append(metrics.auc(fpr, tpr))\n",
    "            n_tree.append(n)\n",
    "            learning_rate.append(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\",500)\n",
    "auc_df_xgb = pd.DataFrame({'n_tree' : n_tree,\n",
    "                           'learning_rate' : learning_rate,\n",
    "                           'auc' : auc})\n",
    "auc_df_xgb.groupby(['n_tree', 'learning_rate']).agg({'auc' : ['mean', 'std']}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "auc = []\n",
    "n_tree = []\n",
    "learning_rate = []\n",
    "max_depth = []\n",
    "gamma = []\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_test = X_test.as_matrix()\n",
    "    \n",
    "    # Upsample minority class in training data using SMOTE\n",
    "    X_train, y_train = SMOTE(random_state = 123).fit_resample(X_train, y_train)\n",
    "    \n",
    "    for n in [50, 100, 150, 200]:\n",
    "        for i in [0.001, 0.01, 0.1, 0.2]:\n",
    "            xgb = XGBClassifier(n_estimators = n, learning_rate = i)\n",
    "            xgb.fit(X_train, y_train)\n",
    "            y_pred = xgb.predict_proba(X_test)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred[:, 1])\n",
    "            auc.append(metrics.auc(fpr, tpr))\n",
    "            n_tree.append(n)\n",
    "            learning_rate.append(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>n_tree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.606251</td>\n",
       "      <td>0.014326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.628908</td>\n",
       "      <td>0.011074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.655569</td>\n",
       "      <td>0.008712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.655005</td>\n",
       "      <td>0.012898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.612367</td>\n",
       "      <td>0.008634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.637608</td>\n",
       "      <td>0.009196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.652436</td>\n",
       "      <td>0.014568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.648919</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>150</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.612825</td>\n",
       "      <td>0.008647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.641870</td>\n",
       "      <td>0.004790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.649725</td>\n",
       "      <td>0.012645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>150</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.643999</td>\n",
       "      <td>0.009858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.615092</td>\n",
       "      <td>0.008874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.646313</td>\n",
       "      <td>0.003311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.647530</td>\n",
       "      <td>0.013670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.640547</td>\n",
       "      <td>0.013710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_tree learning_rate       auc          \n",
       "                             mean       std\n",
       "0      50         0.001  0.606251  0.014326\n",
       "1      50         0.010  0.628908  0.011074\n",
       "2      50         0.100  0.655569  0.008712\n",
       "3      50         0.200  0.655005  0.012898\n",
       "4     100         0.001  0.612367  0.008634\n",
       "5     100         0.010  0.637608  0.009196\n",
       "6     100         0.100  0.652436  0.014568\n",
       "7     100         0.200  0.648919  0.008475\n",
       "8     150         0.001  0.612825  0.008647\n",
       "9     150         0.010  0.641870  0.004790\n",
       "10    150         0.100  0.649725  0.012645\n",
       "11    150         0.200  0.643999  0.009858\n",
       "12    200         0.001  0.615092  0.008874\n",
       "13    200         0.010  0.646313  0.003311\n",
       "14    200         0.100  0.647530  0.013670\n",
       "15    200         0.200  0.640547  0.013710"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\",500)\n",
    "auc_df_xgb = pd.DataFrame({'n_tree' : n_tree,\n",
    "                           'learning_rate' : learning_rate,\n",
    "                           'auc' : auc})\n",
    "auc_df_xgb.groupby(['n_tree', 'learning_rate']).agg({'auc' : ['mean', 'std']}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
